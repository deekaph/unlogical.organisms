---
layout: post
title: Some Historical Perspective on the Future of AI
categories: notes
---

I remember installing Windows 95 when it first came out.

After sitting through floppy flipping 13x3.5" disks and a whole lot of very slow ATA transfers, you could load the Plus CD-ROM (if you were lucky enough to have a drive for such fanciness) and under a directory labelled "funstuff" find a file called "weezer" and good god almight holy shit and what the fuck, there was a *video* playing on a *computer*.

It was potato quality by today's standards but it was nevertheless a moving video picture and audio being transported as data across a data storage medium and then managed by a CPU to display in color to an end viewer.

I knew right then that this was the future. I posited that cable will become as antiquated as broadcast television and that "the future would be on demand". I even developed a system where viewers could consume it using some kind of digital tokens that could be either purchased or earned by viewing ads. People would opt for watching a series of commercials in order to watch a show - to bank them - or if they had the means, could purchase the credits and never have to watch commercials again.

Hell of an idea right? Too bad nobody took it seriously. I was, after all, just a teenage hacker kiddie who obviously did not understand the obvious constraints such a system would require.

"Just think of the bandwidth that would take!" I was condescendingly told by a teacher. "To transmit even a single video would require either compression technology or bandwidth increases orders of magnitude over where we are now, probably both, and that doesn't even take into consideration the storage requirements to host all the videos of the world, or the processing time and memory needs required to serve and manage it all at both the sending and receiving ends."

And, they weren't wrong really - all of those things have indeed improved by orders of magnitude. "High Speed Modems" at the time were still needing to be left connected in a one to one interface over copper telephone lines overnight to download a single jpeg, hard drives cost around $1.20/MB and every single other aspect of computing was prohibitively expensive to scale at the time.

The nerd in me was able to see past that. I reasoned that fifty years prior to that we decided to drop the first two digits of the year to save two bites on the date field and ended up causing the Y2K problem. It seemed ridiculous in hindsight, but at the time those two kilobytes really added up.

Even in my time as a computer nerd, I remember prowling around my 10MB HDD for the DOS help files so I could free up a few hundred extra KB of precious space. 

So, with this in context now, I feel a responsibility to put it on the record now that I think it is likely the same sort of solutions will evolve. That is to say, that the current technological (and economic) limitations we are facing are bound to eventually become moot, those limitations being namely: 

1. Cost of hardware
2. Energy requirements

On the first account, we are witnessing a kind of capitalist cannibalization circle-jerk infinite money glitch thing that is wholly unsustainable and just reeks of the same vendor financing nonsense that should have crushed the global economy back in 2008. OpenAI "orders" a few hundred billion of servers from Oracle, who then "book those sales" causing a huge surge in share value and Larry Ellison momentarily passes Elon on the race to see who can horde the most moneys. Oracle in turn needs to buy a shitload of GPUs to run all those ChatGPT conversations, so they place a future order for a few hundred billion with NVidia and they book the sale themselves, bumping that share value into an even bigger bubble. 

And then what happens?

This is amazaing:

**NVidia dumps some hundred billion republic credits into OpenAI.**

Brilliant right? All we have to do is assume that OpenAI finds a way to come up with all those hundreds of billions or the whole circle collapses.

But - and just hear me out here - what if a H100 didn't cost fifty fuckin grand? 

What if you didn't need H100's at all?

The biggest objection I think most people have to LLM's and AI in general is that the energy consumption is completely out of control. Meta is building data centers bigger than the island of Manhattan, Google is firing up multiple nuclear reactors, everyone's already inflated electricity bills are blowing up and none of this even addresses the amount of water that gets consumed in this process.

The trillions of dollars being shit out and then regurgitated in the aforementioned economic-human-centipede, the voracious consumption of electricity and water and investment capital in the name of a tidal wave of AI slop... it's no wonder that many are extremely cynical of AI technology.

Right now it might be as hard to imagine as having so much memory to throw at compute that you can just blow an extra *two kilobytes* every time you throw a date stamp on something, or of imagining digital videos being served *on demand* (and flying through the damn air no less), but let's take a moment to imagine that we are able to overcome the memory/compute intensive requirements of our near future AI systems. Where highly adept models with advanced reasoning could run locally, say even on your phone.

Technology has moved incredibly fast over the last century, but mark my words, it is about to go truly parabolic. Where it took twenty or so years to go from "come on you can't *stream* television over modems ya twit" to "I am receiving a 4k show into the palm of my hand wirelessly through the air", I think that once a certain threshold is reached - and we can debate on what that threshold is - then this next shift will all happen in a matter of months.

What fascinates me is that these hundreds of billions (nay, *trillions*) being shoveled into the cash incinerators right now are going to a big problem. Those data centers the size of metropolitan cities... those are going to be a problem.

That's *tech debt*... which is something I have encountered a lot in my career. Hell, we once devised a bespoke solution for intercepting LPT output and then injecting it into a monitoring program which would then rearrange the data and format it for output as a PDF that could be printed as A5 on laser printers because the company had invested in a Unix system back in the 80s and "it is still working and we don't want to change it but we are having a hard time sourcing tractor feed paper, ink spools and parts for our dot matrix printers". 

This was in the mid 2010s.

So it is easy for me to imagine these big players who are dumping *gazillions* into NVidia chips and data centers to be less agile to adopt newer technologies as the appear.

But let's be realistic here. I strongly believe that **necessity is the mother of all invention**, and we can't keep going on like this. What are we going to do, cover the entire surface of the planet in data centers so that Tiktok can barf out bespoke AI ads with the viewer's own face in them as they doomscroll? Because that's where this is all going: just as before, "the future will be on demand."

Your AI slop and the ads that power it will all be on demand, modified in realtime to appeal to whatever is generating the most response from you. 

And under current technology, the only solution to that will be to scale the  entire planet into a server farm... or, find a cheaper way to do it. 

Which will not be very good for NVidia's share price, or Oracle's booked but not yet built server farms. 

It might be so bad for their share price that they may end up suppressing such technologies, or releasing them in stages *a la* Intel in the 90s, where great leaps had already taken place but were being slowly filtered to the consumer market so as to extract the maximum value from upgrades. 

If the Pentium 4 was a direct upgrade from a Pentium 1, then why bother being milked for the P2 and then P3 upgrades?

All this is kind of tinfoil hat territory and purely speculation on my part, but if history has taught me anything, is that this is probably more likely to happen than for Jensen Huang to suddenly one day and say "hey we made a new chip that runs cool, draws less power than an LED, is the size of a pinky nail, outperforms paired H100's and only costs $20 to produce".

¯\_(ツ)_/¯

